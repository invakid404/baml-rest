// ----------------------------------------------------------------------------
//
//  Welcome to Baml! To use this generated code, please run the following:
//
//  $ go get github.com/boundaryml/baml
//
// ----------------------------------------------------------------------------

// This file was generated by BAML: please do not edit it. Instead, edit the
// BAML files and re-generate this code using: baml-cli generate
// You can install baml-cli with:
//  $ go install github.com/boundaryml/baml/baml-cli

package baml_client

var file_map = map[string]string{
  
  "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",  
  "dynamic_test.baml": "class DynamicTestOutput {\n\t// summary string @description(\"Summary\")\n\t@@dynamic\n}\n\nfunction DynamicTest(content: string) -> DynamicTestOutput {\n\tclient CustomSonnet\n\tprompt #\"\n\t\tSummarize the provided content.\n\t\t{{ ctx.output_format }}\n\t\t{{ _.role(\"user\") }}\n\t\t{{ content }}\n\t\"#\n}\n",  
  "generators.baml": "generator target {\n    // Some comment\n    output_type \"go\"\n    output_dir \"../\"\n    // version \"0.204.0\"\n    version \"0.205.0\"\n    client_package_name \"github.com/invakid404/testis\"\n    default_client_mode async\n}\n",  
  "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client CustomSonnet // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    {{ ctx.output_format }}\n\n    {{ _.role(\"user\") }}\n    Extract from this content:\n    {{ resume }}\n  \"#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",  
  "resume2.baml": "// Defining a data model.\nclass Resume2 {\n  name string\n  email string\n  experience string[]\n  skills string[]\n  maikati Resume3 | Resume4\n  bashtati Resume3\n  sestrati Resume4\n  dqdoti ChichoTi\n}\n\nclass Resume3 {\n  name string\n  email string?\n  experience string[]\n  skills string[]\n}\n\nclass Resume4 {\n  testis string?\n}\n\nenum ChichoTi {\n  LelqTi\n  StrinkaTi\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume2(resume: string?, testis: Resume2, arr: string[], arr2: string[]?) -> string {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"openai/gpt-4o\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n",  
}

func getBamlFiles() map[string]string {
  return file_map
}