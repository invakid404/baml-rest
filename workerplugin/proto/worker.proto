syntax = "proto3";

package workerplugin;

option go_package = "github.com/invakid404/baml-rest/workerplugin";

// StreamMode controls how streaming results are processed
enum StreamMode {
  STREAM_MODE_CALL = 0;             // final only, no raw, skip partials
  STREAM_MODE_STREAM = 1;           // partials + final, no raw
  STREAM_MODE_CALL_WITH_RAW = 2;    // final + raw, skip intermediate parsing
  STREAM_MODE_STREAM_WITH_RAW = 3;  // partials + final + raw
}

// Request to call a BAML method
message CallRequest {
  string method_name = 1;
  bytes input_json = 2;        // JSON-encoded input struct (includes __baml_options__ if present)
  StreamMode stream_mode = 3;  // Controls partial forwarding and raw collection
}

// Streaming result
message StreamResult {
  enum Kind {
    STREAM = 0;
    FINAL = 1;
    ERROR = 2;
    HEARTBEAT = 3;  // Signals first byte received from LLM (filtered at pool level)
  }
  Kind kind = 1;
  bytes data_json = 2;         // JSON-encoded stream/final data
  string raw = 3;              // Raw LLM response text (populated on FINAL)
  string error = 4;            // Error message (populated on ERROR)
  string stacktrace = 5;       // Stacktrace (populated on ERROR, when available)
  bool reset = 6;              // When true, client should discard accumulated state (retry occurred)
}

// Health check
message Empty {}

message HealthResponse {
  bool healthy = 1;
}

// Metrics response - contains serialized Prometheus metrics
message MetricsResponse {
  // Serialized prometheus MetricFamily protos
  // Use prometheus client_model to deserialize
  repeated bytes metric_families = 1;
}

// GC response - contains memory stats before/after GC
message GCResponse {
  uint64 heap_alloc_before = 1;
  uint64 heap_alloc_after = 2;
  uint64 heap_released = 3;
}

// Request to parse raw LLM output
message ParseRequest {
  string method_name = 1;
  bytes input_json = 2;        // JSON-encoded input (contains "raw" and optional "__baml_options__")
}

// Parse response
message ParseResponse {
  bytes data_json = 1;         // JSON-encoded parsed result
  string error = 2;            // Error message (if parsing failed)
  string stacktrace = 3;       // Stacktrace (if parsing failed, when available)
}

// Goroutines request - for getting pprof data from worker
message GetGoroutinesRequest {
  string filter = 1;  // Optional comma-separated filter patterns (case-insensitive)
}

// Goroutines response - pprof goroutine data
message GetGoroutinesResponse {
  int32 total_count = 1;             // Total number of goroutines
  int32 match_count = 2;             // Number of goroutines matching filter
  repeated string matched_stacks = 3; // Stack traces of matching goroutines
}

// Worker service
service Worker {
  // Streaming call - returns stream of results
  // Used for both streaming and non-streaming calls
  rpc CallStream(CallRequest) returns (stream StreamResult);

  // Health check
  rpc Health(Empty) returns (HealthResponse);

  // Get Prometheus metrics from worker process
  rpc GetMetrics(Empty) returns (MetricsResponse);

  // Trigger garbage collection and release memory to OS
  rpc TriggerGC(Empty) returns (GCResponse);

  // Parse raw LLM output into structured data
  rpc Parse(ParseRequest) returns (ParseResponse);

  // Get goroutine pprof data from worker process
  rpc GetGoroutines(GetGoroutinesRequest) returns (GetGoroutinesResponse);
}
